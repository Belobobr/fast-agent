{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "d788f156",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Downloading artifacts: 100%|██████████| 5/5 [00:00<00:00, 524.83it/s]\n",
      "2025/07/06 17:08:46 INFO mlflow.tracking.fluent: Active model is set to the logged model with ID: m-94ea81a0b1994bb4a4917fc22bdc87d5\n",
      "2025/07/06 17:08:46 INFO mlflow.tracking.fluent: Use `mlflow.set_active_model` to set the active model to a different one if needed.\n",
      "2025/07/06 17:08:46 INFO mlflow.tracking.fluent: Active model is set to the logged model with ID: m-94ea81a0b1994bb4a4917fc22bdc87d5\n",
      "2025/07/06 17:08:46 INFO mlflow.tracking.fluent: Use `mlflow.set_active_model` to set the active model to a different one if needed.\n",
      "2025/07/06 17:08:46 INFO mlflow.models.evaluation.evaluators.default: Computing model predictions.\n",
      "2025/07/06 17:08:51 INFO mlflow.models.evaluation.default_evaluator: Testing metrics on first row...\n",
      "2025/07/06 17:08:51 WARNING mlflow.metrics.metric_definitions: Failed to load 'toxicity' metric (error: ModuleNotFoundError(\"No module named 'evaluate'\")), skipping metric logging.\n",
      "2025/07/06 17:08:51 WARNING mlflow.models.evaluation.utils.metric: Did not log metric 'toxicity' at index 1 in the `extra_metrics` parameter because it returned None.\n",
      "2025/07/06 17:08:51 WARNING mlflow.metrics.metric_definitions: Failed to import textstat for flesch kincaid metric, skipping metric logging. Please install textstat using 'pip install textstat'.\n",
      "2025/07/06 17:08:51 WARNING mlflow.models.evaluation.utils.metric: Did not log metric 'flesch_kincaid_grade_level' at index 2 in the `extra_metrics` parameter because it returned None.\n",
      "2025/07/06 17:08:51 WARNING mlflow.metrics.metric_definitions: Failed to import textstat for automated readability index metric, skipping metric logging. Please install textstat using 'pip install textstat'.\n",
      "2025/07/06 17:08:51 WARNING mlflow.models.evaluation.utils.metric: Did not log metric 'ari_grade_level' at index 3 in the `extra_metrics` parameter because it returned None.\n",
      "100%|██████████| 1/1 [00:05<00:00,  5.23s/it]\n",
      "2025/07/06 17:08:56 WARNING mlflow.metrics.metric_definitions: Failed to load 'toxicity' metric (error: ModuleNotFoundError(\"No module named 'evaluate'\")), skipping metric logging.\n",
      "2025/07/06 17:08:56 WARNING mlflow.models.evaluation.utils.metric: Did not log metric 'toxicity' at index 1 in the `extra_metrics` parameter because it returned None.\n",
      "2025/07/06 17:08:56 WARNING mlflow.metrics.metric_definitions: Failed to import textstat for flesch kincaid metric, skipping metric logging. Please install textstat using 'pip install textstat'.\n",
      "2025/07/06 17:08:56 WARNING mlflow.models.evaluation.utils.metric: Did not log metric 'flesch_kincaid_grade_level' at index 2 in the `extra_metrics` parameter because it returned None.\n",
      "2025/07/06 17:08:56 WARNING mlflow.metrics.metric_definitions: Failed to import textstat for automated readability index metric, skipping metric logging. Please install textstat using 'pip install textstat'.\n",
      "2025/07/06 17:08:56 WARNING mlflow.models.evaluation.utils.metric: Did not log metric 'ari_grade_level' at index 3 in the `extra_metrics` parameter because it returned None.\n",
      "100%|██████████| 2/2 [00:05<00:00,  2.70s/it]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "See aggregated evaluation results below: \n",
      "{'latency/mean': 2.387356162071228, 'latency/variance': 0.01065148637771074, 'latency/p90': 2.4699209928512573, 'exact_match/v1': 0.0, 'answer_correctness/v1/mean': 4.5, 'answer_correctness/v1/variance': 0.25, 'answer_correctness/v1/p90': 4.9}\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Downloading artifacts: 100%|██████████| 1/1 [00:00<00:00, 215.95it/s]\n",
      "Downloading artifacts: 100%|██████████| 1/1 [00:00<00:00, 273.08it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "See evaluation table below: \n",
      "            inputs                                       ground_truth  \\\n",
      "0  What is MLflow?  MLflow is an open-source platform for managing...   \n",
      "1   What is Spark?  Apache Spark is an open-source, distributed co...   \n",
      "\n",
      "                                             outputs   latency  token_count  \\\n",
      "0  MLflow is an open-source platform developed by...  2.490562           54   \n",
      "1  Spark is an open-source distributed general-pu...  2.284150           31   \n",
      "\n",
      "   answer_correctness/v1/score  \\\n",
      "0                            5   \n",
      "1                            4   \n",
      "\n",
      "                 answer_correctness/v1/justification  \n",
      "0  The output provided by the model is correct. I...  \n",
      "1  The output provided by the model is mostly cor...  \n",
      "🏃 View run sincere-fawn-947 at: http://127.0.0.1:5001/#/experiments/0/runs/7cecbc79c558404d9c5cd2da7bd37cf8\n",
      "🧪 View experiment at: http://127.0.0.1:5001/#/experiments/0\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "\n",
       "<div>\n",
       "  <style scoped>\n",
       "  button {\n",
       "    border: none;\n",
       "    border-radius: 4px;\n",
       "    background-color: rgb(34, 114, 180);\n",
       "    font-family: -apple-system, \"system-ui\", \"Segoe UI\", Roboto, \"Helvetica Neue\", Arial;\n",
       "    font-size: 13px;\n",
       "    color: white;\n",
       "    margin-top: 8px;\n",
       "    margin-bottom: 8px;\n",
       "    padding: 8px 16px;\n",
       "    cursor: pointer;\n",
       "  }\n",
       "  button:hover {\n",
       "    background-color: rgb(66, 153, 224);\n",
       "  }\n",
       "  </style>\n",
       "  <button\n",
       "    onclick=\"\n",
       "        const display = this.nextElementSibling.style.display;\n",
       "        const isCollapsed = display === 'none';\n",
       "        this.nextElementSibling.style.display = isCollapsed ? null : 'none';\n",
       "\n",
       "        const verb = isCollapsed ? 'Collapse' : 'Expand';\n",
       "        this.innerText = `${verb} MLflow Trace`;\n",
       "    \"\n",
       "  >Collapse MLflow Trace</button>\n",
       "  <iframe\n",
       "    id=\"trace-renderer\"\n",
       "    style=\"width: 100%; height: 500px; border: none; resize: vertical;\"\n",
       "    src=\"http://127.0.0.1:5001/static-files/lib/notebook-trace-renderer/index.html?trace_id=3ec1ba087f874939927a36eac7cac2d3&amp;experiment_id=0&amp;trace_id=fb1caeddc4b645da90b447e4951212f0&amp;experiment_id=0&amp;version=3.1.1\"\n",
       "  />\n",
       "</div>\n"
      ],
      "text/plain": [
       "[Trace(trace_id=3ec1ba087f874939927a36eac7cac2d3), Trace(trace_id=fb1caeddc4b645da90b447e4951212f0)]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "import mlflow\n",
    "import openai\n",
    "import os\n",
    "import pandas as pd\n",
    "from getpass import getpass\n",
    "from mlflow.metrics import latency\n",
    "from mlflow.metrics.genai import answer_correctness\n",
    "\n",
    "client = openai.OpenAI(\n",
    "    api_key=os.getenv('OPENAI_API_KEY'),\n",
    "    base_url=os.getenv('OPENAI_BASE_URL')\n",
    ")\n",
    "\n",
    "eval_data = pd.DataFrame(\n",
    "    {\n",
    "        \"inputs\": [\n",
    "            \"What is MLflow?\",\n",
    "            \"What is Spark?\",\n",
    "        ],\n",
    "        \"ground_truth\": [\n",
    "            \"MLflow is an open-source platform for managing the end-to-end machine learning (ML) \"\n",
    "            \"lifecycle. It was developed by Databricks, a company that specializes in big data and \"\n",
    "            \"machine learning solutions. MLflow is designed to address the challenges that data \"\n",
    "            \"scientists and machine learning engineers face when developing, training, and deploying \"\n",
    "            \"machine learning models.\",\n",
    "            \"Apache Spark is an open-source, distributed computing system designed for big data \"\n",
    "            \"processing and analytics. It was developed in response to limitations of the Hadoop \"\n",
    "            \"MapReduce computing model, offering improvements in speed and ease of use. Spark \"\n",
    "            \"provides libraries for various tasks such as data ingestion, processing, and analysis \"\n",
    "            \"through its components like Spark SQL for structured data, Spark Streaming for \"\n",
    "            \"real-time data processing, and MLlib for machine learning tasks\",\n",
    "        ],\n",
    "    }\n",
    ")\n",
    "\n",
    "with mlflow.start_run() as run:\n",
    "    system_prompt = \"Answer the following question in two sentences\"\n",
    "    # Wrap \"gpt-4\" as an MLflow model.\n",
    "    logged_model_info = mlflow.openai.log_model(\n",
    "        model=\"gpt-4\",\n",
    "        task=openai.chat.completions,\n",
    "        name=\"model\",\n",
    "        messages=[\n",
    "            {\"role\": \"system\", \"content\": system_prompt},\n",
    "            {\"role\": \"user\", \"content\": \"{question}\"},\n",
    "        ],\n",
    "    )\n",
    "\n",
    "    # Use predefined question-answering metrics to evaluate our model.\n",
    "    results = mlflow.evaluate(\n",
    "        logged_model_info.model_uri,\n",
    "        eval_data,\n",
    "        targets=\"ground_truth\",\n",
    "        model_type=\"question-answering\",\n",
    "        extra_metrics=[\n",
    "            answer_correctness(),\n",
    "            latency(),\n",
    "        ],\n",
    "    )\n",
    "    print(f\"See aggregated evaluation results below: \\n{results.metrics}\")\n",
    "\n",
    "    # Evaluation result for each data record is available in `results.tables`.\n",
    "    eval_table = results.tables[\"eval_results_table\"]\n",
    "    print(f\"See evaluation table below: \\n{eval_table}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "4baff705",
   "metadata": {},
   "outputs": [],
   "source": [
    "professionalism_example_score_2 = mlflow.metrics.genai.EvaluationExample(\n",
    "    input=\"What is MLflow?\",\n",
    "    output=(\n",
    "        \"MLflow is like your friendly neighborhood toolkit for managing your machine learning projects. It helps \"\n",
    "        \"you track experiments, package your code and models, and collaborate with your team, making the whole ML \"\n",
    "        \"workflow smoother. It's like your Swiss Army knife for machine learning!\"\n",
    "    ),\n",
    "    score=2,\n",
    "    justification=(\n",
    "        \"The response is written in a casual tone. It uses contractions, filler words such as 'like', and \"\n",
    "        \"exclamation points, which make it sound less professional. \"\n",
    "    ),\n",
    ")\n",
    "professionalism_example_score_4 = mlflow.metrics.genai.EvaluationExample(\n",
    "    input=\"What is MLflow?\",\n",
    "    output=(\n",
    "        \"MLflow is an open-source platform for managing the end-to-end machine learning (ML) lifecycle. It was \"\n",
    "        \"developed by Databricks, a company that specializes in big data and machine learning solutions. MLflow is \"\n",
    "        \"designed to address the challenges that data scientists and machine learning engineers face when \"\n",
    "        \"developing, training, and deploying machine learning models.\",\n",
    "    ),\n",
    "    score=4,\n",
    "    justification=(\"The response is written in a formal language and a neutral tone. \"),\n",
    ")\n",
    "\n",
    "\n",
    "professionalism = mlflow.metrics.genai.make_genai_metric(\n",
    "    name=\"professionalism\",\n",
    "    definition=(\n",
    "        \"Professionalism refers to the use of a formal, respectful, and appropriate style of communication that is \"\n",
    "        \"tailored to the context and audience. It often involves avoiding overly casual language, slang, or \"\n",
    "        \"colloquialisms, and instead using clear, concise, and respectful language.\"\n",
    "    ),\n",
    "    grading_prompt=(\n",
    "        \"Professionalism: If the answer is written using a professional tone, below are the details for different scores: \"\n",
    "        \"- Score 0: Language is extremely casual, informal, and may include slang or colloquialisms. Not suitable for \"\n",
    "        \"professional contexts.\"\n",
    "        \"- Score 1: Language is casual but generally respectful and avoids strong informality or slang. Acceptable in \"\n",
    "        \"some informal professional settings.\"\n",
    "        \"- Score 2: Language is overall formal but still have casual words/phrases. Borderline for professional contexts.\"\n",
    "        \"- Score 3: Language is balanced and avoids extreme informality or formality. Suitable for most professional contexts. \"\n",
    "        \"- Score 4: Language is noticeably formal, respectful, and avoids casual elements. Appropriate for formal \"\n",
    "        \"business or academic settings. \"\n",
    "    ),\n",
    "    examples=[professionalism_example_score_2, professionalism_example_score_4],\n",
    "    model=\"openai:/gpt-4o-mini\",\n",
    "    parameters={\"temperature\": 0.0},\n",
    "    aggregations=[\"mean\", \"variance\"],\n",
    "    greater_is_better=True,\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "ff51444c",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Downloading artifacts: 100%|██████████| 5/5 [00:00<00:00, 444.48it/s]\n",
      "2025/07/06 17:22:50 INFO mlflow.tracking.fluent: Active model is set to the logged model with ID: m-193fa6f33ec24ebeb48ebdc821a9f321\n",
      "2025/07/06 17:22:50 INFO mlflow.tracking.fluent: Use `mlflow.set_active_model` to set the active model to a different one if needed.\n",
      "2025/07/06 17:22:50 INFO mlflow.tracking.fluent: Active model is set to the logged model with ID: m-193fa6f33ec24ebeb48ebdc821a9f321\n",
      "2025/07/06 17:22:50 INFO mlflow.tracking.fluent: Use `mlflow.set_active_model` to set the active model to a different one if needed.\n",
      "2025/07/06 17:22:50 INFO mlflow.models.evaluation.evaluators.default: Computing model predictions.\n",
      "2025/07/06 17:22:56 INFO mlflow.models.evaluation.default_evaluator: Testing metrics on first row...\n",
      "2025/07/06 17:22:56 WARNING mlflow.metrics.metric_definitions: Failed to load 'toxicity' metric (error: ModuleNotFoundError(\"No module named 'evaluate'\")), skipping metric logging.\n",
      "2025/07/06 17:22:56 WARNING mlflow.models.evaluation.utils.metric: Did not log metric 'toxicity' at index 1 in the `extra_metrics` parameter because it returned None.\n",
      "2025/07/06 17:22:56 WARNING mlflow.metrics.metric_definitions: Failed to import textstat for flesch kincaid metric, skipping metric logging. Please install textstat using 'pip install textstat'.\n",
      "2025/07/06 17:22:56 WARNING mlflow.models.evaluation.utils.metric: Did not log metric 'flesch_kincaid_grade_level' at index 2 in the `extra_metrics` parameter because it returned None.\n",
      "2025/07/06 17:22:56 WARNING mlflow.metrics.metric_definitions: Failed to import textstat for automated readability index metric, skipping metric logging. Please install textstat using 'pip install textstat'.\n",
      "2025/07/06 17:22:56 WARNING mlflow.models.evaluation.utils.metric: Did not log metric 'ari_grade_level' at index 3 in the `extra_metrics` parameter because it returned None.\n",
      "100%|██████████| 1/1 [00:03<00:00,  3.91s/it]\n",
      "100%|██████████| 1/1 [00:01<00:00,  1.80s/it]\n",
      "2025/07/06 17:23:01 WARNING mlflow.metrics.metric_definitions: Failed to load 'toxicity' metric (error: ModuleNotFoundError(\"No module named 'evaluate'\")), skipping metric logging.\n",
      "2025/07/06 17:23:01 WARNING mlflow.models.evaluation.utils.metric: Did not log metric 'toxicity' at index 1 in the `extra_metrics` parameter because it returned None.\n",
      "2025/07/06 17:23:01 WARNING mlflow.metrics.metric_definitions: Failed to import textstat for flesch kincaid metric, skipping metric logging. Please install textstat using 'pip install textstat'.\n",
      "2025/07/06 17:23:01 WARNING mlflow.models.evaluation.utils.metric: Did not log metric 'flesch_kincaid_grade_level' at index 2 in the `extra_metrics` parameter because it returned None.\n",
      "2025/07/06 17:23:01 WARNING mlflow.metrics.metric_definitions: Failed to import textstat for automated readability index metric, skipping metric logging. Please install textstat using 'pip install textstat'.\n",
      "2025/07/06 17:23:01 WARNING mlflow.models.evaluation.utils.metric: Did not log metric 'ari_grade_level' at index 3 in the `extra_metrics` parameter because it returned None.\n",
      "100%|██████████| 2/2 [00:05<00:00,  2.57s/it]\n",
      "100%|██████████| 2/2 [00:02<00:00,  1.08s/it]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "See aggregated evaluation results below: \n",
      "{'latency/mean': 2.6847835779190063, 'latency/variance': 0.3293514773020121, 'latency/p90': 3.143896794319153, 'exact_match/v1': 0.0, 'answer_correctness/v1/mean': 4.5, 'answer_correctness/v1/variance': 0.25, 'answer_correctness/v1/p90': 4.9, 'professionalism/v1/mean': 4.0, 'professionalism/v1/variance': 0.0}\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Downloading artifacts: 100%|██████████| 1/1 [00:00<00:00, 225.79it/s]\n",
      "Downloading artifacts: 100%|██████████| 1/1 [00:00<00:00, 252.26it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "See evaluation table below: \n",
      "            inputs                                       ground_truth  \\\n",
      "0  What is MLflow?  MLflow is an open-source platform for managing...   \n",
      "1   What is Spark?  Apache Spark is an open-source, distributed co...   \n",
      "\n",
      "                                             outputs   latency  token_count  \\\n",
      "0  MLflow is an open-source platform developed by...  3.258675           55   \n",
      "1  Spark is an open-source, distributed computing...  2.110892           41   \n",
      "\n",
      "   answer_correctness/v1/score  \\\n",
      "0                            5   \n",
      "1                            4   \n",
      "\n",
      "                 answer_correctness/v1/justification  \\\n",
      "0  The output provided by the model is correct. I...   \n",
      "1  The output provided by the model is mostly cor...   \n",
      "\n",
      "   professionalism/v1/score                   professionalism/v1/justification  \n",
      "0                         4  The response is written in a formal and respec...  \n",
      "1                         4  The response is written in a formal and respec...  \n",
      "🏃 View run marvelous-donkey-594 at: http://127.0.0.1:5001/#/experiments/0/runs/42aabd4b79ee44cca77855425cb9963e\n",
      "🧪 View experiment at: http://127.0.0.1:5001/#/experiments/0\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "\n",
       "<div>\n",
       "  <style scoped>\n",
       "  button {\n",
       "    border: none;\n",
       "    border-radius: 4px;\n",
       "    background-color: rgb(34, 114, 180);\n",
       "    font-family: -apple-system, \"system-ui\", \"Segoe UI\", Roboto, \"Helvetica Neue\", Arial;\n",
       "    font-size: 13px;\n",
       "    color: white;\n",
       "    margin-top: 8px;\n",
       "    margin-bottom: 8px;\n",
       "    padding: 8px 16px;\n",
       "    cursor: pointer;\n",
       "  }\n",
       "  button:hover {\n",
       "    background-color: rgb(66, 153, 224);\n",
       "  }\n",
       "  </style>\n",
       "  <button\n",
       "    onclick=\"\n",
       "        const display = this.nextElementSibling.style.display;\n",
       "        const isCollapsed = display === 'none';\n",
       "        this.nextElementSibling.style.display = isCollapsed ? null : 'none';\n",
       "\n",
       "        const verb = isCollapsed ? 'Collapse' : 'Expand';\n",
       "        this.innerText = `${verb} MLflow Trace`;\n",
       "    \"\n",
       "  >Collapse MLflow Trace</button>\n",
       "  <iframe\n",
       "    id=\"trace-renderer\"\n",
       "    style=\"width: 100%; height: 500px; border: none; resize: vertical;\"\n",
       "    src=\"http://127.0.0.1:5001/static-files/lib/notebook-trace-renderer/index.html?trace_id=5ffd171c1f5c447caca3c6f13d6df38c&amp;experiment_id=0&amp;trace_id=3fee295aa23641eab987688303083c16&amp;experiment_id=0&amp;version=3.1.1\"\n",
       "  />\n",
       "</div>\n"
      ],
      "text/plain": [
       "[Trace(trace_id=5ffd171c1f5c447caca3c6f13d6df38c), Trace(trace_id=3fee295aa23641eab987688303083c16)]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "client = openai.OpenAI(\n",
    "    api_key=os.getenv('OPENAI_API_KEY'),\n",
    "    base_url=os.getenv('OPENAI_BASE_URL')\n",
    ")\n",
    "\n",
    "eval_data = pd.DataFrame(\n",
    "    {\n",
    "        \"inputs\": [\n",
    "            \"What is MLflow?\",\n",
    "            \"What is Spark?\",\n",
    "        ],\n",
    "        \"ground_truth\": [\n",
    "            \"MLflow is an open-source platform for managing the end-to-end machine learning (ML) \"\n",
    "            \"lifecycle. It was developed by Databricks, a company that specializes in big data and \"\n",
    "            \"machine learning solutions. MLflow is designed to address the challenges that data \"\n",
    "            \"scientists and machine learning engineers face when developing, training, and deploying \"\n",
    "            \"machine learning models.\",\n",
    "            \"Apache Spark is an open-source, distributed computing system designed for big data \"\n",
    "            \"processing and analytics. It was developed in response to limitations of the Hadoop \"\n",
    "            \"MapReduce computing model, offering improvements in speed and ease of use. Spark \"\n",
    "            \"provides libraries for various tasks such as data ingestion, processing, and analysis \"\n",
    "            \"through its components like Spark SQL for structured data, Spark Streaming for \"\n",
    "            \"real-time data processing, and MLlib for machine learning tasks\",\n",
    "        ],\n",
    "    }\n",
    ")\n",
    "\n",
    "with mlflow.start_run() as run:\n",
    "    system_prompt = \"Answer the following question in two sentences\"\n",
    "    # Wrap \"gpt-4\" as an MLflow model.\n",
    "    logged_model_info = mlflow.openai.log_model(\n",
    "        model=\"gpt-4\",\n",
    "        task=openai.chat.completions,\n",
    "        name=\"model\",\n",
    "        messages=[\n",
    "            {\"role\": \"system\", \"content\": system_prompt},\n",
    "            {\"role\": \"user\", \"content\": \"{question}\"},\n",
    "        ],\n",
    "    )\n",
    "\n",
    "    # Use predefined question-answering metrics to evaluate our model.\n",
    "    results = mlflow.evaluate(\n",
    "        logged_model_info.model_uri,\n",
    "        eval_data,\n",
    "        targets=\"ground_truth\",\n",
    "        model_type=\"question-answering\",\n",
    "        extra_metrics=[\n",
    "            answer_correctness(),\n",
    "            latency(),\n",
    "            professionalism\n",
    "        ],\n",
    "    )\n",
    "    print(f\"See aggregated evaluation results below: \\n{results.metrics}\")\n",
    "\n",
    "    # Evaluation result for each data record is available in `results.tables`.\n",
    "    eval_table = results.tables[\"eval_results_table\"]\n",
    "    print(f\"See evaluation table below: \\n{eval_table}\")"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "fast-agent",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.11"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
