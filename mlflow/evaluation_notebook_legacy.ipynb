{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "d788f156",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2025/07/06 22:14:36 INFO mlflow.store.model_registry.abstract_store: Waiting up to 300 seconds for prompt version to finish creation. Prompt name: gpt-4-system-prompot, version 1\n",
      "Downloading artifacts: 100%|██████████| 5/5 [00:00<00:00, 687.37it/s]\n",
      "2025/07/06 22:14:37 INFO mlflow.tracking.fluent: Active model is set to the logged model with ID: m-a2ccca652c274e6685470d3843337b06\n",
      "2025/07/06 22:14:37 INFO mlflow.tracking.fluent: Use `mlflow.set_active_model` to set the active model to a different one if needed.\n",
      "2025/07/06 22:14:37 INFO mlflow.models.evaluation.evaluators.default: Computing model predictions.\n",
      "2025/07/06 22:14:42 INFO mlflow.models.evaluation.default_evaluator: Testing metrics on first row...\n",
      "2025/07/06 22:14:42 WARNING mlflow.metrics.metric_definitions: Failed to load 'toxicity' metric (error: ModuleNotFoundError(\"No module named 'evaluate'\")), skipping metric logging.\n",
      "2025/07/06 22:14:42 WARNING mlflow.models.evaluation.utils.metric: Did not log metric 'toxicity' at index 1 in the `extra_metrics` parameter because it returned None.\n",
      "2025/07/06 22:14:42 WARNING mlflow.metrics.metric_definitions: Failed to import textstat for flesch kincaid metric, skipping metric logging. Please install textstat using 'pip install textstat'.\n",
      "2025/07/06 22:14:42 WARNING mlflow.models.evaluation.utils.metric: Did not log metric 'flesch_kincaid_grade_level' at index 2 in the `extra_metrics` parameter because it returned None.\n",
      "2025/07/06 22:14:42 WARNING mlflow.metrics.metric_definitions: Failed to import textstat for automated readability index metric, skipping metric logging. Please install textstat using 'pip install textstat'.\n",
      "2025/07/06 22:14:42 WARNING mlflow.models.evaluation.utils.metric: Did not log metric 'ari_grade_level' at index 3 in the `extra_metrics` parameter because it returned None.\n",
      "100%|██████████| 1/1 [00:04<00:00,  4.05s/it]\n",
      "2025/07/06 22:14:46 WARNING mlflow.metrics.metric_definitions: Failed to load 'toxicity' metric (error: ModuleNotFoundError(\"No module named 'evaluate'\")), skipping metric logging.\n",
      "2025/07/06 22:14:46 WARNING mlflow.models.evaluation.utils.metric: Did not log metric 'toxicity' at index 1 in the `extra_metrics` parameter because it returned None.\n",
      "2025/07/06 22:14:46 WARNING mlflow.metrics.metric_definitions: Failed to import textstat for flesch kincaid metric, skipping metric logging. Please install textstat using 'pip install textstat'.\n",
      "2025/07/06 22:14:46 WARNING mlflow.models.evaluation.utils.metric: Did not log metric 'flesch_kincaid_grade_level' at index 2 in the `extra_metrics` parameter because it returned None.\n",
      "2025/07/06 22:14:46 WARNING mlflow.metrics.metric_definitions: Failed to import textstat for automated readability index metric, skipping metric logging. Please install textstat using 'pip install textstat'.\n",
      "2025/07/06 22:14:46 WARNING mlflow.models.evaluation.utils.metric: Did not log metric 'ari_grade_level' at index 3 in the `extra_metrics` parameter because it returned None.\n",
      "100%|██████████| 2/2 [00:04<00:00,  2.32s/it]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "See aggregated evaluation results below: \n",
      "{'latency/mean': 2.570592164993286, 'latency/variance': 0.023621233035612477, 'latency/p90': 2.693545770645142, 'exact_match/v1': 0.0, 'answer_correctness/v1/mean': 4.5, 'answer_correctness/v1/variance': 0.25, 'answer_correctness/v1/p90': 4.9}\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Downloading artifacts: 100%|██████████| 1/1 [00:00<00:00, 292.22it/s]\n",
      "Downloading artifacts: 100%|██████████| 1/1 [00:00<00:00, 272.09it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "See evaluation table below: \n",
      "            inputs                                       ground_truth  \\\n",
      "0  What is MLflow?  MLflow is an open-source platform for managing...   \n",
      "1   What is Spark?  Apache Spark is an open-source, distributed co...   \n",
      "\n",
      "                                             outputs   latency  token_count  \\\n",
      "0  MLflow is an open-source platform designed to ...  2.724284           51   \n",
      "1  Spark is an open-source, distributed computing...  2.416900           34   \n",
      "\n",
      "   answer_correctness/v1/score  \\\n",
      "0                            5   \n",
      "1                            4   \n",
      "\n",
      "                 answer_correctness/v1/justification  \n",
      "0  The output provided by the model is correct. I...  \n",
      "1  The output provided by the model is mostly cor...  \n",
      "🏃 View run secretive-skink-96 at: http://127.0.0.1:5001/#/experiments/483845778527648610/runs/50f8ad68cb664c34a311742f27473145\n",
      "🧪 View experiment at: http://127.0.0.1:5001/#/experiments/483845778527648610\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "\n",
       "<div>\n",
       "  <style scoped>\n",
       "  button {\n",
       "    border: none;\n",
       "    border-radius: 4px;\n",
       "    background-color: rgb(34, 114, 180);\n",
       "    font-family: -apple-system, \"system-ui\", \"Segoe UI\", Roboto, \"Helvetica Neue\", Arial;\n",
       "    font-size: 13px;\n",
       "    color: white;\n",
       "    margin-top: 8px;\n",
       "    margin-bottom: 8px;\n",
       "    padding: 8px 16px;\n",
       "    cursor: pointer;\n",
       "  }\n",
       "  button:hover {\n",
       "    background-color: rgb(66, 153, 224);\n",
       "  }\n",
       "  </style>\n",
       "  <button\n",
       "    onclick=\"\n",
       "        const display = this.nextElementSibling.style.display;\n",
       "        const isCollapsed = display === 'none';\n",
       "        this.nextElementSibling.style.display = isCollapsed ? null : 'none';\n",
       "\n",
       "        const verb = isCollapsed ? 'Collapse' : 'Expand';\n",
       "        this.innerText = `${verb} MLflow Trace`;\n",
       "    \"\n",
       "  >Collapse MLflow Trace</button>\n",
       "  <iframe\n",
       "    id=\"trace-renderer\"\n",
       "    style=\"width: 100%; height: 500px; border: none; resize: vertical;\"\n",
       "    src=\"http://127.0.0.1:5001/static-files/lib/notebook-trace-renderer/index.html?trace_id=75cbc65225e14216852bbdc4b0918dcb&amp;experiment_id=483845778527648610&amp;trace_id=4bed276967da40fb9c3af3693dd98b12&amp;experiment_id=483845778527648610&amp;version=3.1.1\"\n",
       "  />\n",
       "</div>\n"
      ],
      "text/plain": [
       "[Trace(trace_id=75cbc65225e14216852bbdc4b0918dcb), Trace(trace_id=4bed276967da40fb9c3af3693dd98b12)]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "import mlflow\n",
    "import openai\n",
    "import os\n",
    "import pandas as pd\n",
    "from getpass import getpass\n",
    "from mlflow.metrics import latency\n",
    "from mlflow.metrics.genai import answer_correctness\n",
    "\n",
    "client = openai.OpenAI(\n",
    "    api_key=os.getenv('OPENAI_API_KEY'),\n",
    "    base_url=os.getenv('OPENAI_BASE_URL')\n",
    ")\n",
    "\n",
    "eval_data = pd.DataFrame(\n",
    "    {\n",
    "        \"inputs\": [\n",
    "            \"What is MLflow?\",\n",
    "            \"What is Spark?\",\n",
    "        ],\n",
    "        \"ground_truth\": [\n",
    "            \"MLflow is an open-source platform for managing the end-to-end machine learning (ML) \"\n",
    "            \"lifecycle. It was developed by Databricks, a company that specializes in big data and \"\n",
    "            \"machine learning solutions. MLflow is designed to address the challenges that data \"\n",
    "            \"scientists and machine learning engineers face when developing, training, and deploying \"\n",
    "            \"machine learning models.\",\n",
    "            \"Apache Spark is an open-source, distributed computing system designed for big data \"\n",
    "            \"processing and analytics. It was developed in response to limitations of the Hadoop \"\n",
    "            \"MapReduce computing model, offering improvements in speed and ease of use. Spark \"\n",
    "            \"provides libraries for various tasks such as data ingestion, processing, and analysis \"\n",
    "            \"through its components like Spark SQL for structured data, Spark Streaming for \"\n",
    "            \"real-time data processing, and MLlib for machine learning tasks\",\n",
    "        ],\n",
    "    }\n",
    ")\n",
    "\n",
    "system_prompt = mlflow.genai.register_prompt(\n",
    "    name=\"gpt-4-system-prompot\",\n",
    "    template=\"Answer the following question in two sentences\",\n",
    "    commit_message=\"Initial version of chatbot\",\n",
    ")\n",
    "\n",
    "with mlflow.start_run() as run:\n",
    "    # system_prompt = \"Answer the following question in two sentences\"\n",
    "    # Wrap \"gpt-4\" as an MLflow model.\n",
    "    logged_model_info = mlflow.openai.log_model(\n",
    "        model=\"gpt-4\",\n",
    "        task=openai.chat.completions,\n",
    "        name=\"model\",\n",
    "        messages=[\n",
    "            {\"role\": \"system\", \"content\": system_prompt.template},\n",
    "            {\"role\": \"user\", \"content\": \"{question}\"},\n",
    "        ],\n",
    "    )\n",
    "\n",
    "    # Use predefined question-answering metrics to evaluate our model.\n",
    "    results = mlflow.evaluate(\n",
    "        logged_model_info.model_uri,\n",
    "        eval_data,\n",
    "        targets=\"ground_truth\",\n",
    "        model_type=\"question-answering\",\n",
    "        extra_metrics=[\n",
    "            answer_correctness(),\n",
    "            latency(),\n",
    "        ],\n",
    "    )\n",
    "    print(f\"See aggregated evaluation results below: \\n{results.metrics}\")\n",
    "\n",
    "    # Evaluation result for each data record is available in `results.tables`.\n",
    "    eval_table = results.tables[\"eval_results_table\"]\n",
    "    print(f\"See evaluation table below: \\n{eval_table}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "4baff705",
   "metadata": {},
   "outputs": [],
   "source": [
    "professionalism_example_score_2 = mlflow.metrics.genai.EvaluationExample(\n",
    "    input=\"What is MLflow?\",\n",
    "    output=(\n",
    "        \"MLflow is like your friendly neighborhood toolkit for managing your machine learning projects. It helps \"\n",
    "        \"you track experiments, package your code and models, and collaborate with your team, making the whole ML \"\n",
    "        \"workflow smoother. It's like your Swiss Army knife for machine learning!\"\n",
    "    ),\n",
    "    score=2,\n",
    "    justification=(\n",
    "        \"The response is written in a casual tone. It uses contractions, filler words such as 'like', and \"\n",
    "        \"exclamation points, which make it sound less professional. \"\n",
    "    ),\n",
    ")\n",
    "professionalism_example_score_4 = mlflow.metrics.genai.EvaluationExample(\n",
    "    input=\"What is MLflow?\",\n",
    "    output=(\n",
    "        \"MLflow is an open-source platform for managing the end-to-end machine learning (ML) lifecycle. It was \"\n",
    "        \"developed by Databricks, a company that specializes in big data and machine learning solutions. MLflow is \"\n",
    "        \"designed to address the challenges that data scientists and machine learning engineers face when \"\n",
    "        \"developing, training, and deploying machine learning models.\",\n",
    "    ),\n",
    "    score=4,\n",
    "    justification=(\"The response is written in a formal language and a neutral tone. \"),\n",
    ")\n",
    "\n",
    "\n",
    "professionalism = mlflow.metrics.genai.make_genai_metric(\n",
    "    name=\"professionalism\",\n",
    "    definition=(\n",
    "        \"Professionalism refers to the use of a formal, respectful, and appropriate style of communication that is \"\n",
    "        \"tailored to the context and audience. It often involves avoiding overly casual language, slang, or \"\n",
    "        \"colloquialisms, and instead using clear, concise, and respectful language.\"\n",
    "    ),\n",
    "    grading_prompt=(\n",
    "        \"Professionalism: If the answer is written using a professional tone, below are the details for different scores: \"\n",
    "        \"- Score 0: Language is extremely casual, informal, and may include slang or colloquialisms. Not suitable for \"\n",
    "        \"professional contexts.\"\n",
    "        \"- Score 1: Language is casual but generally respectful and avoids strong informality or slang. Acceptable in \"\n",
    "        \"some informal professional settings.\"\n",
    "        \"- Score 2: Language is overall formal but still have casual words/phrases. Borderline for professional contexts.\"\n",
    "        \"- Score 3: Language is balanced and avoids extreme informality or formality. Suitable for most professional contexts. \"\n",
    "        \"- Score 4: Language is noticeably formal, respectful, and avoids casual elements. Appropriate for formal \"\n",
    "        \"business or academic settings. \"\n",
    "    ),\n",
    "    examples=[professionalism_example_score_2, professionalism_example_score_4],\n",
    "    model=\"openai:/gpt-4o-mini\",\n",
    "    parameters={\"temperature\": 0.0},\n",
    "    aggregations=[\"mean\", \"variance\"],\n",
    "    greater_is_better=True,\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "23dbcc9c",
   "metadata": {},
   "outputs": [],
   "source": [
    "# system_prompt = mlflow.genai.register_prompt(\n",
    "#     name=\"gpt-4-system-prompot\",\n",
    "#     template=\"Answer the following question in two sentences\",\n",
    "#     commit_message=\"Initial version of chatbot\",\n",
    "# )"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "id": "ff51444c",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2025/07/06 22:30:49 WARNING mlflow.tracking.fluent: Multiple LoggedModels found with name 'gpt-4', setting the latest one as active model.\n",
      "2025/07/06 22:30:49 INFO mlflow.tracking.fluent: Active model is set to the logged model with ID: m-1b68c33cd82b44f2bf02fff8f894c385\n",
      "Downloading artifacts: 100%|██████████| 5/5 [00:00<00:00, 696.17it/s]\n",
      "2025/07/06 22:30:51 INFO mlflow.tracking.fluent: Active model is set to the logged model with ID: m-840c4406366948d397e88fc2b95798b9\n",
      "2025/07/06 22:30:51 INFO mlflow.tracking.fluent: Use `mlflow.set_active_model` to set the active model to a different one if needed.\n",
      "2025/07/06 22:30:51 INFO mlflow.models.evaluation.evaluators.default: Computing model predictions.\n",
      "2025/07/06 22:30:52 INFO mlflow.models.evaluation.default_evaluator: Testing metrics on first row...\n",
      "2025/07/06 22:30:52 WARNING mlflow.metrics.metric_definitions: Failed to load 'toxicity' metric (error: ModuleNotFoundError(\"No module named 'evaluate'\")), skipping metric logging.\n",
      "2025/07/06 22:30:52 WARNING mlflow.models.evaluation.utils.metric: Did not log metric 'toxicity' at index 1 in the `extra_metrics` parameter because it returned None.\n",
      "2025/07/06 22:30:52 WARNING mlflow.metrics.metric_definitions: Failed to import textstat for flesch kincaid metric, skipping metric logging. Please install textstat using 'pip install textstat'.\n",
      "2025/07/06 22:30:52 WARNING mlflow.models.evaluation.utils.metric: Did not log metric 'flesch_kincaid_grade_level' at index 2 in the `extra_metrics` parameter because it returned None.\n",
      "2025/07/06 22:30:52 WARNING mlflow.metrics.metric_definitions: Failed to import textstat for automated readability index metric, skipping metric logging. Please install textstat using 'pip install textstat'.\n",
      "2025/07/06 22:30:52 WARNING mlflow.models.evaluation.utils.metric: Did not log metric 'ari_grade_level' at index 3 in the `extra_metrics` parameter because it returned None.\n",
      "100%|██████████| 1/1 [00:00<00:00,  1.69it/s]\n",
      "100%|██████████| 1/1 [00:00<00:00,  1.69it/s]\n",
      "2025/07/06 22:30:53 WARNING mlflow.metrics.metric_definitions: Failed to load 'toxicity' metric (error: ModuleNotFoundError(\"No module named 'evaluate'\")), skipping metric logging.\n",
      "2025/07/06 22:30:53 WARNING mlflow.models.evaluation.utils.metric: Did not log metric 'toxicity' at index 1 in the `extra_metrics` parameter because it returned None.\n",
      "2025/07/06 22:30:53 WARNING mlflow.metrics.metric_definitions: Failed to import textstat for flesch kincaid metric, skipping metric logging. Please install textstat using 'pip install textstat'.\n",
      "2025/07/06 22:30:53 WARNING mlflow.models.evaluation.utils.metric: Did not log metric 'flesch_kincaid_grade_level' at index 2 in the `extra_metrics` parameter because it returned None.\n",
      "2025/07/06 22:30:53 WARNING mlflow.metrics.metric_definitions: Failed to import textstat for automated readability index metric, skipping metric logging. Please install textstat using 'pip install textstat'.\n",
      "2025/07/06 22:30:53 WARNING mlflow.models.evaluation.utils.metric: Did not log metric 'ari_grade_level' at index 3 in the `extra_metrics` parameter because it returned None.\n",
      "100%|██████████| 2/2 [00:00<00:00,  3.30it/s]\n",
      "100%|██████████| 2/2 [00:00<00:00,  3.23it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "See aggregated evaluation results below: \n",
      "{'latency/mean': 0.6516404151916504, 'latency/variance': 3.064817519771168e-07, 'latency/p90': 0.6520833015441895, 'exact_match/v1': 0.0, 'answer_correctness/v1/mean': 5.0, 'answer_correctness/v1/variance': 0.0, 'answer_correctness/v1/p90': 5.0, 'professionalism/v1/mean': 4.0, 'professionalism/v1/variance': 0.0}\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Downloading artifacts: 100%|██████████| 1/1 [00:00<00:00, 209.16it/s]\n",
      "Downloading artifacts: 100%|██████████| 1/1 [00:00<00:00, 237.13it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "See evaluation table below: \n",
      "            inputs                                       ground_truth  \\\n",
      "0  What is MLflow?  MLflow is an open-source platform for managing...   \n",
      "1   What is Spark?  Apache Spark is an open-source, distributed co...   \n",
      "\n",
      "                                             outputs   latency  token_count  \\\n",
      "0  MLflow is an open-source platform developed by...  0.651087           58   \n",
      "1  Apache Spark is an open-source, distributed co...  0.652194           48   \n",
      "\n",
      "   answer_correctness/v1/score  \\\n",
      "0                            5   \n",
      "1                            5   \n",
      "\n",
      "                 answer_correctness/v1/justification  \\\n",
      "0  The output provided by the model is correct. I...   \n",
      "1  The output provided by the model is correct. I...   \n",
      "\n",
      "   professionalism/v1/score                   professionalism/v1/justification  \n",
      "0                         4  The response is written in a formal and respec...  \n",
      "1                         4  The response is written in a formal and respec...  \n",
      "🏃 View run melodic-squirrel-683 at: http://127.0.0.1:5001/#/experiments/991215029613644238/runs/4fcec74eba83414b93309d32c1fff18f\n",
      "🧪 View experiment at: http://127.0.0.1:5001/#/experiments/991215029613644238\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "\n",
       "<div>\n",
       "  <style scoped>\n",
       "  button {\n",
       "    border: none;\n",
       "    border-radius: 4px;\n",
       "    background-color: rgb(34, 114, 180);\n",
       "    font-family: -apple-system, \"system-ui\", \"Segoe UI\", Roboto, \"Helvetica Neue\", Arial;\n",
       "    font-size: 13px;\n",
       "    color: white;\n",
       "    margin-top: 8px;\n",
       "    margin-bottom: 8px;\n",
       "    padding: 8px 16px;\n",
       "    cursor: pointer;\n",
       "  }\n",
       "  button:hover {\n",
       "    background-color: rgb(66, 153, 224);\n",
       "  }\n",
       "  </style>\n",
       "  <button\n",
       "    onclick=\"\n",
       "        const display = this.nextElementSibling.style.display;\n",
       "        const isCollapsed = display === 'none';\n",
       "        this.nextElementSibling.style.display = isCollapsed ? null : 'none';\n",
       "\n",
       "        const verb = isCollapsed ? 'Collapse' : 'Expand';\n",
       "        this.innerText = `${verb} MLflow Trace`;\n",
       "    \"\n",
       "  >Collapse MLflow Trace</button>\n",
       "  <iframe\n",
       "    id=\"trace-renderer\"\n",
       "    style=\"width: 100%; height: 500px; border: none; resize: vertical;\"\n",
       "    src=\"http://127.0.0.1:5001/static-files/lib/notebook-trace-renderer/index.html?trace_id=0d8eb4abc6e74fc689f11712871cedce&amp;experiment_id=991215029613644238&amp;trace_id=a825cec0fb7647aa97699d25a981797a&amp;experiment_id=991215029613644238&amp;version=3.1.1\"\n",
       "  />\n",
       "</div>\n"
      ],
      "text/plain": [
       "[Trace(trace_id=0d8eb4abc6e74fc689f11712871cedce), Trace(trace_id=a825cec0fb7647aa97699d25a981797a)]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "import mlflow.openai\n",
    "import openai\n",
    "\n",
    "\n",
    "client = openai.OpenAI(\n",
    "    api_key=os.getenv('OPENAI_API_KEY'),\n",
    "    base_url=os.getenv('OPENAI_BASE_URL')\n",
    ")\n",
    "\n",
    "eval_data = pd.DataFrame(\n",
    "    {\n",
    "        \"inputs\": [\n",
    "            \"What is MLflow?\",\n",
    "            \"What is Spark?\",\n",
    "        ],\n",
    "        \"ground_truth\": [\n",
    "            \"MLflow is an open-source platform for managing the end-to-end machine learning (ML) \"\n",
    "            \"lifecycle. It was developed by Databricks, a company that specializes in big data and \"\n",
    "            \"machine learning solutions. MLflow is designed to address the challenges that data \"\n",
    "            \"scientists and machine learning engineers face when developing, training, and deploying \"\n",
    "            \"machine learning models.\",\n",
    "            \"Apache Spark is an open-source, distributed computing system designed for big data \"\n",
    "            \"processing and analytics. It was developed in response to limitations of the Hadoop \"\n",
    "            \"MapReduce computing model, offering improvements in speed and ease of use. Spark \"\n",
    "            \"provides libraries for various tasks such as data ingestion, processing, and analysis \"\n",
    "            \"through its components like Spark SQL for structured data, Spark Streaming for \"\n",
    "            \"real-time data processing, and MLlib for machine learning tasks\",\n",
    "        ],\n",
    "    }\n",
    ")\n",
    "\n",
    "\n",
    "mlflow.set_experiment(\"my-prompt-app\")\n",
    "mlflow.set_active_model(name=\"gpt-4\")\n",
    "mlflow.openai.autolog()\n",
    "\n",
    "prompt_version = \"prompts:/gpt-4-system-prompot/3\"\n",
    "\n",
    "prompt = mlflow.genai.load_prompt(prompt_version)\n",
    "\n",
    "\n",
    "with mlflow.start_run() as run:\n",
    "    # Wrap \"gpt-4\" as an MLflow model.\n",
    "    logged_model_info = mlflow.openai.log_model(\n",
    "        model=\"gpt-4\",\n",
    "        task=openai.chat.completions,\n",
    "        name=\"gpt-4\",\n",
    "        # prompts=[system_prompt],\n",
    "        prompts=[prompt_version],\n",
    "        messages=[\n",
    "            {\"role\": \"system\", \"content\": prompt.template},\n",
    "            {\"role\": \"user\", \"content\": \"{question}\"},\n",
    "        ],\n",
    "    )\n",
    "\n",
    "    # Use predefined question-answering metrics to evaluate our model.\n",
    "    results = mlflow.evaluate(\n",
    "        logged_model_info.model_uri,\n",
    "        eval_data,\n",
    "        targets=\"ground_truth\",\n",
    "        model_type=\"question-answering\",\n",
    "        extra_metrics=[\n",
    "            answer_correctness(),\n",
    "            latency(),\n",
    "            professionalism\n",
    "        ],\n",
    "    )\n",
    "    print(f\"See aggregated evaluation results below: \\n{results.metrics}\")\n",
    "\n",
    "    # Evaluation result for each data record is available in `results.tables`.\n",
    "    eval_table = results.tables[\"eval_results_table\"]\n",
    "    print(f\"See evaluation table below: \\n{eval_table}\")"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "fast-agent",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.11"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
